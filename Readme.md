// author: hcy
// date: 2024.6.21

// 这个phase-switch文件夹是要对多写问题进行一个简易化的微实现
// 多写问题在polarDB-MP被关键抽象成了Transaction fusion, Buffer fusion, Lock fusion
// 1. Transaction fusion是对数据可见性的判断, 每个节点维护了一个TIT表, 目的是当事务访问一个数据项时，需要判断这个数据项当前的版本是否是可见的
// 这通过CTS来维护，如果CTS是还未填充的数据，则需要通过数据项上的txn_id来去对应的节点去查看当前这个事务是否是活跃的
// 这在多节点场景下的确定数据可见性的实现方式

// 2. Buffer fusion是多写问题的关键, 他处理了两个节点同时修改一个数据项的问题
// 首先当一个数据页想要去fetch一个page时，他首先检查本地的Page Lock Table, 如果已经持有了这个page的锁，那么无需再去远程节点获取这个page latch
// 如果没有持有这个page的锁，那么需要去远程节点获取这个page latch
// 使用完这个page后，需要释放这个page latch, 释放page latch之前, 必须要将这个page的修改写入到远程内存池, 
// 并将所有持有这个数据页的节点中的页表中相关的valid项置为false

// 3. Lock fusion包括了PLock和RLock, PLock处理多个节点并发访问页面的冲突, 他和Buffer fusion共同处理这个问题
// RLock处理行锁的问题, 他在低隔离级别下不使用，因此在这个探究中是无所谓的

// 梳理完PolarDB-MP的实现之后，再来看他的问题在哪, 多个计算节点对数据页访问的抖动是影响多写性能的关键
// 首先, PolarDB-MP和Taurus-MM的实验中, 假设有N个计算节点, 他们都是将一个表逻辑上划分成N+1个数据分区
// 对于N个分区, 每个分区的query都发送给一个计算节点, 而对于第N+1个数据分区, 每个计算节点都可以处理
// 通过控制第N+1的分区的大小来变化share-data的比例, 实验表明share-data的比例很低时, 多写系统近乎可以线性拓展
// 然而, 这种划分方式在实际场景中是不实际的, 因为在这种处理方式下, 前N个分区的数据由计算节点独占, 而无法处理涉及第0个分区和第1个分区的事务
// 这在实际系统不可能因为他无法处理全表扫描的事务, 也因此在实际系统中, share data的比例应该为100%, 而实验效果表明扩展性并不好, 不是线性拓展
// 影响性能的关键是 对于多个计算节点并发访问同一个数据页, 需要Page latch的协调（全局加锁）和数据页的同步, 这是让性能下降的关键

// 尽管可以通过上层路由层简单的将访问一个分区的事务发送到同一个计算节点，从而来减少数据页的跨节点传输, 
// 但是对于跨分片的事务, 很难将他们都交由一个计算节点，因为这会造成负载的不均衡从而收敛为单主架构
// 在大部分分布式数据库中, 将表分成N个分区, 事务可能会访问单个分区的事务, 也可能会访问跨分区的事务
// 假设对于N个计算节点, 具有M个数据分区, 上层有一个路由负责将query分发到对应的计算节点
// 考虑三个计算节点和三个数据分区
// 例如如果操作序列是 W1(T1(P1)), W2(T1(P1,P2)), W1(T2(P1)), W2(T2(P1,P2)), W1(T3(P1)), W2(T3(P1,P2))
// 如果对P1访问的是同一个数据页, 则在上述的6个事务中, 共需要5次页面的传输. (每次都需要去远程加页锁, 并将数据页从其他数据节点拉到本地, 这造成了数据页访问的抖动).
// 而如果执行顺序为 W1(T1(P1)), W1(T2(P1)), W1(T3(P1)), W2(T1(P1,P2)), W2(T2(P1,P2)), W2(T3(P1,P2))
// 在这个执行顺序中, 只需要一次页面的传输W1(T3(P1)) -> W2(T1(P1,P2))
// 通过改变多个计算节点拿到数据页的顺序, 可以减少数据页的传输次数, 从而提高性能

run 
./workload 0.8 0.1 1000000 4

./compute_server smallbank eager 1 1 1 1 

